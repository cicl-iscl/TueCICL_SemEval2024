{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from flair.data import Sentence, Label\n",
    "from flair.models import SequenceTagger\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.exists('src/data/subtaskC_train.jsonl'):\n",
    "    !wget https://s3.tebi.io/winkler.stuff/subtaskC_dev.jsonl -O src/data/subtaskC_dev.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskC_train = []\n",
    "with open (\"src/data/subtaskC_train.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line:\n",
    "            parsed = json.loads(line)\n",
    "            taskC_train.append((parsed[\"text\"], parsed[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-09 21:14:42,814 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load(\"flair/pos-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I\"/PRP\n",
      "\"love\"/VBP\n",
      "\"Berlin\"/NNP\n",
      "\".\"/.\n"
     ]
    }
   ],
   "source": [
    "sentence = Sentence(\"I love Berlin.\")\n",
    "# predict NER tags\n",
    "tagger.predict(sentence)\n",
    "for entity in sentence.get_labels('pos'):\n",
    "    e: Label = entity\n",
    "    print(e.shortstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 22/3649 [03:58<10:55:33, 10.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m tokens \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      8\u001b[0m sentence \u001b[38;5;241m=\u001b[39m Sentence(tokens)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m entities \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mget_labels(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m [e\u001b[38;5;241m.\u001b[39mshortstring \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m entities]\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/flair/models/sequence_tagger_model.py:490\u001b[0m, in \u001b[0;36mSequenceTagger.predict\u001b[0;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode, force_token_predictions)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;66;03m# get features from forward propagation\u001b[39;00m\n\u001b[0;32m--> 490\u001b[0m sentence_tensor, lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(sentence_tensor, lengths)\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# remove previously predicted labels of this type\u001b[39;00m\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/flair/models/sequence_tagger_model.py:287\u001b[0m, in \u001b[0;36mSequenceTagger._prepare_tensors\u001b[0;34m(self, data_points)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_tensors\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_points: Union[List[Sentence], Sentence]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mLongTensor]:\n\u001b[1;32m    286\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [data_points] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_points, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m data_points\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# make a zero-padded tensor for the whole sentence\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     lengths, sentence_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_padded_tensor_for_batch(sentences)\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/flair/embeddings/token.py:100\u001b[0m, in \u001b[0;36mStackedEmbeddings.embed\u001b[0;34m(self, sentences, static_embeddings)\u001b[0m\n\u001b[1;32m     97\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [sentences]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings:\n\u001b[0;32m--> 100\u001b[0m     \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/flair/embeddings/base.py:50\u001b[0m, in \u001b[0;36mEmbeddings.embed\u001b[0;34m(self, data_points)\u001b[0m\n\u001b[1;32m     47\u001b[0m     data_points \u001b[38;5;241m=\u001b[39m [data_points]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_everything_embedded(data_points):\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_embeddings_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_points\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/flair/embeddings/token.py:818\u001b[0m, in \u001b[0;36mFlairEmbeddings._add_embeddings_internal\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    815\u001b[0m end_marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# get hidden states from language model\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_representation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_marker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchars_per_chunk\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfine_tune:\n\u001b[1;32m    823\u001b[0m     all_hidden_states_in_lm \u001b[38;5;241m=\u001b[39m all_hidden_states_in_lm\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/flair/models/language_model.py:160\u001b[0m, in \u001b[0;36mLanguageModel.get_representation\u001b[0;34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[1;32m    159\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 160\u001b[0m     rnn_output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     output_parts\u001b[38;5;241m.\u001b[39mappend(rnn_output)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# concatenate all chunks to make final output\u001b[39;00m\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/flair/models/language_model.py:88\u001b[0m, in \u001b[0;36mLanguageModel.forward\u001b[0;34m(self, input, hidden, ordered_sequence_lengths, decode)\u001b[0m\n\u001b[1;32m     86\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (h,)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(output)\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/torch/nn/modules/rnn.py:879\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    876\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    882\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    883\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_tokens, data_pos = [], []\n",
    "\n",
    "for text, label in tqdm(taskC_train):\n",
    "    sentences = sent_tokenize(text, language=\"english\")\n",
    "    t, p = [], []\n",
    "    for s in sentences:\n",
    "        tokens = s.split()\n",
    "        sentence = Sentence(tokens)\n",
    "        tagger.predict(sentence)\n",
    "        entities = sentence.get_labels(\"pos\")\n",
    "        labels = [e.shortstring for e in entities]\n",
    "        for result in labels:\n",
    "            spl = result.split(\"/\")\n",
    "            pos = spl[-1]\n",
    "            word = \"/\".join(spl[:-1]).replace('\"', \"\")\n",
    "            t.append(word)\n",
    "            p.append(pos)\n",
    "    data_tokens.append(t)\n",
    "    data_pos.append(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- \t\t\t :\n",
      "Strengths: \t\t\t NN\n",
      "* \t\t\t NFP\n",
      "Outperforms \t\t\t VBZ\n",
      "ALIGN \t\t\t NNP\n",
      "in \t\t\t IN\n",
      "supervised \t\t\t JJ\n",
      "entity \t\t\t NN\n",
      "linking \t\t\t VBG\n",
      "task \t\t\t NN\n",
      "which \t\t\t WDT\n",
      "suggests \t\t\t VBZ\n",
      "that \t\t\t IN\n",
      "the \t\t\t DT\n",
      "proposed \t\t\t VBN\n",
      "framework \t\t\t NN\n",
      "improves \t\t\t VBZ\n",
      "representations \t\t\t NNS\n",
      "of \t\t\t IN\n",
      "text \t\t\t NN\n",
      "and \t\t\t CC\n",
      "knowledge \t\t\t NN\n",
      "that \t\t\t WDT\n",
      "are \t\t\t VBP\n",
      "learned \t\t\t VBN\n",
      "jointly. \t\t\t RB\n",
      "* \t\t\t NFP\n",
      "Direct \t\t\t JJ\n",
      "comparison \t\t\t NN\n",
      "with \t\t\t IN\n",
      "closely \t\t\t RB\n",
      "related \t\t\t VBN\n",
      "approach \t\t\t NN\n",
      "using \t\t\t VBG\n",
      "very \t\t\t RB\n",
      "similar \t\t\t JJ\n",
      "input \t\t\t NN\n",
      "data. \t\t\t NN\n",
      "* \t\t\t NFP\n",
      "Analysis \t\t\t NN\n",
      "of \t\t\t IN\n",
      "the \t\t\t DT\n",
      "smoothing \t\t\t VBG\n",
      "parameter \t\t\t NN\n",
      "provides \t\t\t VBZ\n",
      "useful \t\t\t JJ\n",
      "analysis \t\t\t NN\n",
      "since \t\t\t IN\n",
      "impact \t\t\t NN\n",
      "of \t\t\t IN\n",
      "popularity \t\t\t NN\n",
      "is \t\t\t VBZ\n",
      "a \t\t\t DT\n",
      "persistent \t\t\t JJ\n",
      "issue \t\t\t NN\n",
      "in \t\t\t IN\n",
      "entity \t\t\t NN\n",
      "linking. \t\t\t VBN\n",
      "- \t\t\t :\n",
      "Weaknesses: \t\t\t :\n",
      "* \t\t\t NFP\n",
      "Comparison \t\t\t NN\n",
      "with \t\t\t IN\n",
      "ALIGN \t\t\t NNP\n",
      "could \t\t\t MD\n",
      "be \t\t\t VB\n",
      "better. \t\t\t JJR\n",
      "ALIGN \t\t\t NNP\n",
      "used \t\t\t VBN\n",
      "content \t\t\t NN\n",
      "window \t\t\t NN\n",
      "size \t\t\t NN\n",
      "10 \t\t\t CD\n",
      "vs \t\t\t IN\n",
      "this \t\t\t DT\n",
      "paper's \t\t\t JJ\n",
      "5, \t\t\t CD\n",
      "vector \t\t\t NN\n",
      "dimension \t\t\t NN\n",
      "of \t\t\t IN\n",
      "500 \t\t\t CD\n",
      "vs \t\t\t IN\n",
      "this \t\t\t DT\n",
      "paper's \t\t\t NN\n",
      "200. \t\t\t CD\n",
      "Also \t\t\t RB\n",
      "its \t\t\t PRP$\n",
      "not \t\t\t RB\n",
      "clear \t\t\t JJ\n",
      "to \t\t\t IN\n",
      "me \t\t\t PRP\n",
      "whether \t\t\t IN\n",
      "N(e_j) \t\t\t NNP\n",
      "includes \t\t\t VBZ\n",
      "only \t\t\t JJ\n",
      "entities \t\t\t NNS\n",
      "that \t\t\t WDT\n",
      "link \t\t\t VBP\n",
      "to \t\t\t IN\n",
      "the \t\t\t DT\n",
      "mention \t\t\t NN\n",
      "or \t\t\t CC\n",
      "also \t\t\t RB\n",
      "includes \t\t\t VBZ\n",
      "other \t\t\t JJ\n",
      "related \t\t\t JJ\n",
      "entities. \t\t\t NNS\n",
      "This \t\t\t DT\n",
      "could \t\t\t MD\n",
      "be \t\t\t VB\n",
      "further \t\t\t RB\n",
      "clarified \t\t\t VBN\n",
      "in \t\t\t IN\n",
      "the \t\t\t DT\n",
      "paper. \t\t\t NN\n",
      "Additionally, \t\t\t RB\n",
      "the \t\t\t DT\n",
      "experimental \t\t\t JJ\n",
      "results \t\t\t NNS\n",
      "could \t\t\t MD\n",
      "be \t\t\t VB\n",
      "strengthened \t\t\t VBN\n",
      "by \t\t\t IN\n",
      "including \t\t\t VBG\n",
      "comparisons \t\t\t NNS\n",
      "with \t\t\t IN\n",
      "other \t\t\t JJ\n",
      "state-of-the-art \t\t\t JJ\n",
      "models \t\t\t NNS\n",
      "and \t\t\t CC\n",
      "datasets. \t\t\t NNS\n",
      "While \t\t\t IN\n",
      "the \t\t\t DT\n",
      "proposed \t\t\t VBN\n",
      "framework \t\t\t NN\n",
      "shows \t\t\t VBZ\n",
      "promising \t\t\t JJ\n",
      "results \t\t\t NNS\n",
      "in \t\t\t IN\n",
      "entity \t\t\t NN\n",
      "linking, \t\t\t VBN\n",
      "it \t\t\t PRP\n",
      "would \t\t\t MD\n",
      "be \t\t\t VB\n",
      "informative \t\t\t JJ\n",
      "to \t\t\t TO\n",
      "see \t\t\t VB\n",
      "how \t\t\t WRB\n",
      "it \t\t\t PRP\n",
      "performs \t\t\t VBZ\n",
      "in \t\t\t IN\n",
      "other \t\t\t JJ\n",
      "text \t\t\t NN\n",
      "and \t\t\t CC\n",
      "knowledge \t\t\t NN\n",
      "integration \t\t\t NN\n",
      "tasks. \t\t\t NN\n",
      "Overall, \t\t\t RB\n",
      "the \t\t\t DT\n",
      "paper \t\t\t NN\n",
      "presents \t\t\t VBZ\n",
      "an \t\t\t DT\n",
      "interesting \t\t\t JJ\n",
      "approach \t\t\t NN\n",
      "to \t\t\t TO\n",
      "address \t\t\t VB\n",
      "the \t\t\t DT\n",
      "ambiguity \t\t\t NN\n",
      "in \t\t\t IN\n",
      "entity \t\t\t NN\n",
      "mentions \t\t\t NNS\n",
      "by \t\t\t IN\n",
      "learning \t\t\t VBG\n",
      "multi-prototype \t\t\t NN\n",
      "mention \t\t\t NN\n",
      "embeddings. \t\t\t NN\n",
      "The \t\t\t DT\n",
      "proposed \t\t\t VBN\n",
      "disambiguation \t\t\t NN\n",
      "method \t\t\t NN\n",
      "and \t\t\t CC\n",
      "embedding \t\t\t NN\n",
      "models \t\t\t NNS\n",
      "show \t\t\t VBP\n",
      "high \t\t\t JJ\n",
      "quality \t\t\t NN\n",
      "performance, \t\t\t NN\n",
      "leading \t\t\t VBG\n",
      "to \t\t\t IN\n",
      "state-of-the-art \t\t\t JJ\n",
      "results \t\t\t NNS\n",
      "in \t\t\t IN\n",
      "entity \t\t\t NN\n",
      "linking. \t\t\t VBN\n",
      "With \t\t\t IN\n",
      "some \t\t\t DT\n",
      "minor \t\t\t JJ\n",
      "improvements \t\t\t NNS\n",
      "and \t\t\t CC\n",
      "further \t\t\t JJ\n",
      "analysis, \t\t\t NN\n",
      "this \t\t\t DT\n",
      "work \t\t\t NN\n",
      "has \t\t\t VBZ\n",
      "the \t\t\t DT\n",
      "potential \t\t\t NN\n",
      "to \t\t\t TO\n",
      "make \t\t\t VB\n",
      "a \t\t\t DT\n",
      "significant \t\t\t JJ\n",
      "contribution \t\t\t NN\n",
      "to \t\t\t IN\n",
      "the \t\t\t DT\n",
      "field \t\t\t NN\n",
      "of \t\t\t IN\n",
      "text \t\t\t NN\n",
      "and \t\t\t CC\n",
      "knowledge \t\t\t NN\n",
      "integration. \t\t\t NN\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
