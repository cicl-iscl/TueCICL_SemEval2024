{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import typing as T\n",
    "import string\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106cc6ff0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_json(\"./data/subtaskA_train_monolingual.jsonl\", lines=True)\n",
    "    dev = pd.read_json(\"./data/subtaskA_dev_monolingual.jsonl\", lines=True)\n",
    "    return train, dev\n",
    "\n",
    "train, dev = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITESPACE = \"<WS>\"\n",
    "PUNCTUATION = \"<PUNCT>\"\n",
    "DIGIT = \"<DIGIT>\"\n",
    "UNK = \"<UNK>\"\n",
    "SENT_TERMINATE = \"<SENT_TERMINATE>\"\n",
    "\n",
    "BOS = \"<BOS>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "def map_char(char: str):\n",
    "    sentence_ending = [\".\", \"!\", \"?\"]\n",
    "    if char.isspace():\n",
    "        return WHITESPACE\n",
    "    if char in sentence_ending:\n",
    "        return SENT_TERMINATE\n",
    "    if char in string.punctuation:\n",
    "        return PUNCTUATION\n",
    "    if char in string.digits:\n",
    "        return DIGIT\n",
    "    if char not in string.printable:\n",
    "        return UNK\n",
    "    return char\n",
    "\n",
    "def build_vocab(train_set: pd.DataFrame):\n",
    "    vocab = set()\n",
    "    for _, series in train_set.iterrows():\n",
    "        text: str = series[\"text\"]\n",
    "        tokens: T.List[str] = [*text.lower().strip()]\n",
    "        tokens = [map_char(token) for token in tokens]\n",
    "        for token in tokens:\n",
    "            vocab.add(token)\n",
    "    vocab = list(vocab)\n",
    "    \n",
    "    vocab.append(BOS)\n",
    "    vocab.append(EOS)\n",
    "    vocab.append(PAD)\n",
    "    \n",
    "    word2idx = {\n",
    "        word: idx for idx, word in enumerate(vocab)\n",
    "    }\n",
    "    idx2word = {\n",
    "        idx: word for idx, word in enumerate(vocab)\n",
    "    }\n",
    "    return word2idx, idx2word, vocab\n",
    "\n",
    "def get_vocab():\n",
    "    fp = \"./data/charlm_vocab.pkl\"\n",
    "    try:\n",
    "        with open(fp, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        train, _ = get_data()\n",
    "        res = build_vocab(train)\n",
    "        with open(fp, \"wb\") as f:\n",
    "            pickle.dump(res, f)\n",
    "        return res\n",
    "    \n",
    "word2idx, idx2word, vocab = get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_tokens(text:str):\n",
    "    tokens: T.List[str] = [*text.lower().strip()]\n",
    "    tokens = [map_char(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(texts: T.List[str], max_len=None, add_special_tokens=True):\n",
    "    tokenized_texts = [get_text_tokens(t) for t in texts]\n",
    "    \n",
    "    longest_len = max([len(t) for t in tokenized_texts])\n",
    "    if (max_len < longest_len):\n",
    "        longest_len = max_len\n",
    "    tokenized_texts = [t[:longest_len] for t in tokenized_texts]\n",
    "    \n",
    "    tokens, attentions = [], []\n",
    "    for tokenized_text in tokenized_texts:\n",
    "        \n",
    "        pad_amount = longest_len - len(tokenized_text)\n",
    "        if add_special_tokens:\n",
    "            tokenized_text = [BOS] + tokenized_text + [EOS]\n",
    "        \n",
    "        tokenized_text += [PAD] * (pad_amount)\n",
    "        tokens.append([word2idx.get(token, UNK) for token in tokenized_text])\n",
    "        attentions.append([1 if token != PAD else 0 for token in tokenized_text])\n",
    "    return torch.tensor(tokens, device=device), torch.tensor(attentions, device=device)\n",
    "\n",
    "def decode(tokens: T.List[T.List[int]]):\n",
    "    return [[idx2word[token] for token in tokenized_text] for tokenized_text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskA_Dataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        if (split == \"train\"):\n",
    "            self.data = pd.read_json(\"./data/subtaskA_train_monolingual.jsonl\", lines=True)\n",
    "        else:\n",
    "            self.data = pd.read_json(\"./data/subtaskA_dev_monolingual.jsonl\", lines=True)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        text, label, _id = item[\"text\"], item[\"label\"], item[\"id\"]\n",
    "        return text, label, _id\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLM(nn.Module):\n",
    "    def __init__(self, vocab_size=None, emb_size=8, hidden_size=1024, num_layers=1) -> None:\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size=hidden_size,\n",
    "            input_size=emb_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier_head = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    \n",
    "    def forward(self, input_ids, attention):\n",
    "        embedded = self.emb(input_ids)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        out = out[:, -1, :]\n",
    "        pred = self.classifier_head(out)\n",
    "        pred = F.log_softmax(pred, dim=1)\n",
    "        return pred\n",
    "        \n",
    "\n",
    "def collate_fn(data):\n",
    "    labels = [i[1] for i in data]\n",
    "    texts = [i[0] for i in data]\n",
    "    ids = [i[2] for  i in data]\n",
    "    max_len = 10_000\n",
    "    input_ids, attentions = tokenize(texts, max_len=max_len)\n",
    "    return input_ids, attentions, torch.tensor(labels, device=device), torch.tensor(ids, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset):\n",
    "    dev_dataloader = DataLoader(dataset, shuffle=False, batch_size=10, collate_fn=collate_fn)\n",
    "    y_pred = []\n",
    "    y_gold = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attentions, labels, _ in dev_dataloader:\n",
    "            out = model(input_ids, attentions)\n",
    "            for i in range(out.shape[0]):\n",
    "                pred = torch.argmax(out[i]).item()\n",
    "                y_pred.append(pred)\n",
    "                y_gold.append(labels[i].item())\n",
    "    \n",
    "    print(classification_report(y_gold, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_checkpoint(model, optimizer, epoch, prefix=\"classifier\"):\n",
    "    try:\n",
    "        os.mkdir(\"checkpoints\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    torch.save(checkpoint, f\"checkpoints/{prefix}_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model=None, optimizer=None, dataloader=None, n_epochs=5, checkpoint_prefix=None, start_epoch=1):\n",
    "    criterion = nn.NLLLoss(reduction=\"mean\")\n",
    "    \n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        with tqdm(total=len(dataloader)) as pbar:\n",
    "            pbar.set_description(f\"Epoch {epoch}\")\n",
    "            losses = []\n",
    "            for input_ids, attentions, labels, _ in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                classifier_out = model(input_ids, attentions)\n",
    "                \n",
    "                # ------------------\n",
    "                # Classifier loss\n",
    "                # ------------------\n",
    "                \n",
    "                loss = criterion(classifier_out, labels)\n",
    "                \n",
    "                if torch.isnan(loss):\n",
    "                    print(\"LOSS IS NAN\")\n",
    "                    continue\n",
    "                                \n",
    "                # ------------------\n",
    "                # Backprop\n",
    "                # ------------------\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.update(1)\n",
    "\n",
    "            print(\"LOSS\", sum(losses) / len(losses))\n",
    "            evaluate(model, TaskA_Dataset(split=\"dev\"))\n",
    "            make_checkpoint(model, optimizer, epoch, prefix=checkpoint_prefix)\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 14970/14970 [8:05:47<00:00,  3.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS 0.6671743715275266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aron/codebase/uniwork/cicl/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aron/codebase/uniwork/cicl/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aron/codebase/uniwork/cicl/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch 1: 100%|██████████| 14970/14970 [8:09:32<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67      2500\n",
      "           1       0.00      0.00      0.00      2500\n",
      "\n",
      "    accuracy                           0.50      5000\n",
      "   macro avg       0.25      0.50      0.33      5000\n",
      "weighted avg       0.25      0.50      0.33      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:  12%|█▏        | 1849/14970 [48:21<5:43:07,  1.57s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# evaluate(model, TaskA_Dataset(split=\"dev\"))\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 31\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader, n_epochs, checkpoint_prefix, start_epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 9\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclassifier_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattentions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 28\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     26\u001b[0m ids \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m  i \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     27\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10_000\u001b[39m\n\u001b[0;32m---> 28\u001b[0m input_ids, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_ids, attentions, torch\u001b[38;5;241m.\u001b[39mtensor(labels, device\u001b[38;5;241m=\u001b[39mdevice), torch\u001b[38;5;241m.\u001b[39mtensor(ids, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[32], line 24\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(texts, max_len, add_special_tokens)\u001b[0m\n\u001b[1;32m     22\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mappend([word2idx\u001b[38;5;241m.\u001b[39mget(token, UNK) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text])\n\u001b[1;32m     23\u001b[0m     attentions\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m!=\u001b[39m PAD \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokenized_text])\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(attentions, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CharLM(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=256,\n",
    "    num_layers=2\n",
    ")\n",
    "model.to(device)\n",
    "start_epoch = 1\n",
    "prefix=\"charclass_256_2\"\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "ds = TaskA_Dataset(split=\"train\")\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "CP = None\n",
    "\n",
    "if CP:\n",
    "    checkpoint_data = torch.load(CP)\n",
    "    model.load_state_dict(checkpoint_data[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint_data[\"optimizer\"])\n",
    "    start_epoch = checkpoint_data[\"epoch\"] + 1\n",
    "    print(\"-------------------------\")\n",
    "    print(\"CHECKPOINT MODEL EVAL\")\n",
    "    print(\"-------------------------\")\n",
    "    # evaluate(model, TaskA_Dataset(split=\"dev\"))\n",
    "    print()\n",
    "\n",
    "losses = train(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    dataloader=loader, \n",
    "    n_epochs=5, \n",
    "    checkpoint_prefix=prefix,\n",
    "    start_epoch=start_epoch\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
