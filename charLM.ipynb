{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import typing as T\n",
    "import string\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_json(\"./data/subtaskA_train_monolingual.jsonl\", lines=True)\n",
    "    dev = pd.read_json(\"./data/subtaskA_dev_monolingual.jsonl\", lines=True)\n",
    "    return train, dev\n",
    "\n",
    "train, dev = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WHITESPACE = \"<WS>\"\n",
    "PUNCTUATION = \"<PUNCT>\"\n",
    "DIGIT = \"<DIGIT>\"\n",
    "UNK = \"<UNK>\"\n",
    "SENT_TERMINATE = \"<SENT_TERMINATE>\"\n",
    "\n",
    "BOS = \"<BOS>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "def map_char(char: str):\n",
    "    sentence_ending = [\".\", \"!\", \"?\"]\n",
    "    if char.isspace():\n",
    "        return WHITESPACE\n",
    "    if char in sentence_ending:\n",
    "        return SENT_TERMINATE\n",
    "    if char in string.punctuation:\n",
    "        return PUNCTUATION\n",
    "    if char in string.digits:\n",
    "        return DIGIT\n",
    "    if char not in string.printable:\n",
    "        return UNK\n",
    "    return char\n",
    "\n",
    "def build_vocab(train_set: pd.DataFrame):\n",
    "    vocab = set()\n",
    "    for _, series in train_set.iterrows():\n",
    "        text: str = series[\"text\"]\n",
    "        tokens: T.List[str] = [*text.lower().strip()]\n",
    "        tokens = [map_char(token) for token in tokens]\n",
    "        for token in tokens:\n",
    "            vocab.add(token)\n",
    "    vocab = list(vocab)\n",
    "    \n",
    "    vocab.append(BOS)\n",
    "    vocab.append(EOS)\n",
    "    vocab.append(PAD)\n",
    "    \n",
    "    word2idx = {\n",
    "        word: idx for idx, word in enumerate(vocab)\n",
    "    }\n",
    "    idx2word = {\n",
    "        idx: word for idx, word in enumerate(vocab)\n",
    "    }\n",
    "    return word2idx, idx2word, vocab\n",
    "\n",
    "def get_vocab():\n",
    "    fp = \"./data/charlm_vocab.pkl\"\n",
    "    try:\n",
    "        with open(fp, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        train, _ = get_data()\n",
    "        res = build_vocab(train)\n",
    "        with open(fp, \"wb\") as f:\n",
    "            pickle.dump(res, f)\n",
    "        return res\n",
    "    \n",
    "word2idx, idx2word, vocab = get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_tokens(text:str):\n",
    "    tokens: T.List[str] = [*text.lower().strip()]\n",
    "    tokens = [map_char(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(texts: T.List[str], max_len=None, add_special_tokens=True):\n",
    "    tokenized_texts = [get_text_tokens(t) for t in texts]\n",
    "    \n",
    "    longest_len = max([len(t) for t in tokenized_texts])\n",
    "    if (max_len < longest_len):\n",
    "        longest_len = max_len\n",
    "    tokenized_texts = [t[:longest_len] for t in tokenized_texts]\n",
    "    \n",
    "    tokens, attentions = [], []\n",
    "    for tokenized_text in tokenized_texts:\n",
    "        \n",
    "        pad_amount = longest_len - len(tokenized_text)\n",
    "        if add_special_tokens:\n",
    "            tokenized_text = [BOS] + tokenized_text + [EOS]\n",
    "        \n",
    "        tokenized_text += [PAD] * (pad_amount)\n",
    "        tokens.append([word2idx[token] for token in tokenized_text])\n",
    "        attentions.append([1 if token != PAD else 0 for token in tokenized_text])\n",
    "    return torch.tensor(tokens, device=device), torch.tensor(attentions, device=device)\n",
    "\n",
    "def decode(tokens: T.List[T.List[int]]):\n",
    "    return [[idx2word[token] for token in tokenized_text] for tokenized_text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskA_Dataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        if (split == \"train\"):\n",
    "            self.data = pd.read_json(\"./data/subtaskA_train_monolingual.jsonl\", lines=True)\n",
    "        else:\n",
    "            self.data = pd.read_json(\"./data/subtaskA_dev_monolingual.jsonl\", lines=True)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        text, label, _id = item[\"text\"], item[\"label\"], item[\"id\"]\n",
    "        return text, label, _id\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAX char ~ 200.000\n",
    "MEAN char ~ 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLM(nn.Module):\n",
    "    def __init__(self, vocab_size=None, emb_size=8, hidden_size=1024, num_layers=1) -> None:\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size=hidden_size,\n",
    "            input_size=emb_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm2lm = nn.Linear(hidden_size, vocab_size)\n",
    "        self.lstm2class = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.emb(input_ids)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        lm_out = self.lstm2lm(out)\n",
    "        lm_out = F.log_softmax(lm_out, dim=-1)\n",
    "        classification_out = self.lstm2class(torch.mean(out, dim=1))\n",
    "        classification_out = F.log_softmax(classification_out, dim=-1)\n",
    "        return lm_out, classification_out\n",
    "\n",
    "def collate_fn(data):\n",
    "    labels = [i[1] for i in data]\n",
    "    texts = [i[0] for i in data]\n",
    "    ids = [i[2] for  i in data]\n",
    "    max_len = 10000\n",
    "    input_ids, attentions = tokenize(texts, max_len=max_len)\n",
    "    return input_ids, attentions, torch.tensor(labels, device=device), torch.tensor(ids, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset):\n",
    "    dev_dataloader = DataLoader(dataset, shuffle=False, batch_size=10, collate_fn=collate_fn)\n",
    "    y_pred = []\n",
    "    y_gold = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attentions, labels, _ in dev_dataloader:\n",
    "            _, out = model(input_ids)\n",
    "            for i in range(out.shape[0]):\n",
    "                pred = torch.argmax(out[i]).item()\n",
    "                y_pred.append(pred)\n",
    "                y_gold.append(labels[i].item())\n",
    "    \n",
    "    print(classification_report(y_gold, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isnan\n",
    "\n",
    "\n",
    "def train(model=None, optimizer=None, dataloader=None, n_epochs=5):\n",
    "    lm_criterion = nn.NLLLoss(reduction=\"mean\")\n",
    "    cl_criterion = nn.NLLLoss(reduction=\"sum\")\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        for i in range(1, n_epochs+1):\n",
    "            pbar.set_description(f\"Epoch {i}\")\n",
    "            for input_ids, attentions, labels, text_ids in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                lm_out, classifier_out = model(input_ids)\n",
    "                loss = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "                \n",
    "                # ------------------\n",
    "                # LM loss\n",
    "                # ------------------\n",
    "                \n",
    "                for i in range(input_ids.shape[0]):\n",
    "                    if labels[i].item() == 0:\n",
    "                        # only train LM on human texts\n",
    "                        y_pred = lm_out[i, :-1]\n",
    "                        y_gold = input_ids[i, 1:]\n",
    "                        loss_update = lm_criterion(y_pred, y_gold)\n",
    "                        if isnan(loss_update.item()):\n",
    "                            print(text_ids[i].item())\n",
    "                            print(input_ids)\n",
    "                            print(y_pred, y_gold)\n",
    "                            return\n",
    "                        loss += loss_update\n",
    "                \n",
    "                # ------------------\n",
    "                # Classifier loss\n",
    "                # ------------------\n",
    "                loss_update = cl_criterion(classifier_out, labels)\n",
    "                loss += loss_update\n",
    "                \n",
    "                # ------------------\n",
    "                # Backprop\n",
    "                # ------------------\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"LOSS\": sum(losses)/len(losses)})\n",
    "                \n",
    "            evaluate(model, TaskA_Dataset(split=\"dev\"))\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.13      0.21      2500\n",
      "           1       0.51      0.90      0.65      2500\n",
      "\n",
      "    accuracy                           0.52      5000\n",
      "   macro avg       0.54      0.52      0.43      5000\n",
      "weighted avg       0.54      0.52      0.43      5000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aron/codebase/uniwork/cicl/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Epoch 1:   0%|          | 47/29940 [00:25<4:26:31,  1.87it/s, LOSS=8.69]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m ds \u001b[38;5;241m=\u001b[39m TaskA_Dataset(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     10\u001b[0m     ds,\n\u001b[1;32m     11\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     13\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader, n_epochs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# ------------------\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# ------------------\u001b[39;00m\n\u001b[1;32m     46\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 47\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codebase/uniwork/cicl/env/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CharLM(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=126\n",
    ")\n",
    "model.to(device)\n",
    "evaluate(model, TaskA_Dataset(split=\"dev\"))\n",
    "optimizer = AdamW(model.parameters(), lr=0.005)\n",
    "ds = TaskA_Dataset(split=\"train\")\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    shuffle=True,\n",
    "    batch_size=4,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "losses = train(model=model, optimizer=optimizer, dataloader=loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
