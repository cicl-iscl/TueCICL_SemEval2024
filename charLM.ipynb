{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "import pandas as pd\n",
    "import typing as T\n",
    "import string\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9089803d90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_json(\"./data/subtaskA_train_monolingual.jsonl\", lines=True)\n",
    "    dev = pd.read_json(\"./data/subtaskA_dev_monolingual.jsonl\", lines=True)\n",
    "    return train, dev\n",
    "\n",
    "train, dev = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4230\n"
     ]
    }
   ],
   "source": [
    "WHITESPACE = \"<WS>\"\n",
    "PUNCTUATION = \"<PUNCT>\"\n",
    "DIGIT = \"<DIGIT>\"\n",
    "UNK = \"<UNK>\"\n",
    "SENT_TERMINATE = \"<SENT_TERMINATE>\"\n",
    "\n",
    "BOS = \"<BOS>\"\n",
    "EOS = \"<EOS>\"\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "CONDENSED_VOCAB = False\n",
    "\n",
    "def map_char(char: str, condense=True):\n",
    "    if not condense:\n",
    "        return char\n",
    "    \n",
    "    sentence_ending = [\".\", \"!\", \"?\"]\n",
    "    if char.isspace():\n",
    "        return WHITESPACE\n",
    "    if char in sentence_ending:\n",
    "        return SENT_TERMINATE\n",
    "    if char in string.punctuation:\n",
    "        return PUNCTUATION\n",
    "    if char in string.digits:\n",
    "        return DIGIT\n",
    "    if char not in string.printable:\n",
    "        return UNK\n",
    "    return char\n",
    "\n",
    "def build_vocab(train_set: pd.DataFrame, condense=True):\n",
    "    vocab = set([UNK])\n",
    "    for _, series in train_set.iterrows():\n",
    "        text: str = series[\"text\"]\n",
    "        tokens: T.List[str] = [*text.lower().strip()]\n",
    "        tokens = [map_char(token, condense=condense) for token in tokens]\n",
    "        for token in tokens:\n",
    "            vocab.add(token)\n",
    "    vocab = list(vocab)\n",
    "\n",
    "    vocab.append(BOS)\n",
    "    vocab.append(EOS)\n",
    "    vocab.append(PAD)\n",
    "    \n",
    "    word2idx = {\n",
    "        word: idx for idx, word in enumerate(vocab)\n",
    "    }\n",
    "    idx2word = {\n",
    "        idx: word for idx, word in enumerate(vocab)\n",
    "    }\n",
    "    return word2idx, idx2word, vocab\n",
    "\n",
    "def get_vocab(condense=True):\n",
    "    fp = \"./data/charlm_vocab.pkl\" if condense else \"./data/charlm_vocab_uncondensed.pkl\"\n",
    "    try:\n",
    "        with open(fp, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    except:\n",
    "        train, _ = get_data()\n",
    "        res = build_vocab(train, condense=condense)\n",
    "        with open(fp, \"wb\") as f:\n",
    "            pickle.dump(res, f)\n",
    "        return res\n",
    "    \n",
    "word2idx, idx2word, vocab = get_vocab(condense=CONDENSED_VOCAB)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_tokens(text:str, condense=False):\n",
    "    tokens: T.List[str] = [*text.lower().strip()]\n",
    "    tokens = [map_char(token, condense=condense) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def tokenize(texts: T.List[str], max_len=None, add_special_tokens=True, condense=False):\n",
    "    tokenized_texts = [get_text_tokens(t, condense=condense) for t in texts]\n",
    "    \n",
    "    longest_len = max([len(t) for t in tokenized_texts])\n",
    "    if max_len and max_len < longest_len:\n",
    "        longest_len = max_len\n",
    "    tokenized_texts = [t[:longest_len] for t in tokenized_texts]\n",
    "    \n",
    "    tokens, attentions = [], []\n",
    "    for tokenized_text in tokenized_texts:\n",
    "        \n",
    "        pad_amount = longest_len - len(tokenized_text)\n",
    "        if add_special_tokens:\n",
    "            tokenized_text = [BOS] + tokenized_text + [EOS]\n",
    "        \n",
    "        tokenized_text += [PAD] * (pad_amount)\n",
    "        tokens.append([word2idx.get(token, word2idx[UNK]) for token in tokenized_text])\n",
    "        attentions.append([1 if token != PAD else 0 for token in tokenized_text])\n",
    "    return torch.tensor(tokens, device=device), torch.tensor(attentions, device=device)\n",
    "\n",
    "def decode(tokens: T.List[T.List[int]]):\n",
    "    return [[idx2word[token] for token in tokenized_text] for tokenized_text in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskA_Dataset(Dataset):\n",
    "    def __init__(self, split=\"train\") -> None:\n",
    "        if split == \"train\":\n",
    "            self.data = pd.read_json(\"./data/subtaskA_train_monolingual.jsonl\", lines=True)\n",
    "        else:\n",
    "            self.data = pd.read_json(\"./data/subtaskA_dev_monolingual.jsonl\", lines=True)\n",
    "\n",
    "    def split_long_items(self, max_len=5000, slide_amount=500):\n",
    "        new_data = []\n",
    "        for _, item in self.data.iterrows():\n",
    "            if len(item[\"text\"]) <= max_len:\n",
    "                continue\n",
    "            pos = 0\n",
    "            window = pos + max_len\n",
    "            while window < len(item[\"text\"]):\n",
    "                new_data.append({\n",
    "                    \"text\": item[\"text\"][pos:window],\n",
    "                    \"label\": item[\"label\"],\n",
    "                    \"id\": item[\"id\"]\n",
    "                })\n",
    "                pos += slide_amount\n",
    "                window += slide_amount\n",
    "        self.data = pd.DataFrame(new_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data.iloc[index]\n",
    "        text, label, _id = item[\"text\"], item[\"label\"], item[\"id\"]\n",
    "        return text, label, _id\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAX char ~ 200.000\n",
    "\n",
    "MEAN char ~ 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLM(nn.Module):\n",
    "    def __init__(self, vocab_size=None, emb_size=8, hidden_size=1024, num_layers=1) -> None:\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            hidden_size=hidden_size,\n",
    "            input_size=emb_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm2lm = nn.Linear(hidden_size, vocab_size)\n",
    "        self.lstm2class = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def _get_means(self, tensors, attentions):\n",
    "        # Function computes embedding averages, but taking out the PAD tokens, as they should not contribute to MEAN.\n",
    "        # Written kinda complicated to avoid copying tensors as much as possible, a previous solution was extremely slow because of\n",
    "        # such copy and slice operations\n",
    "        batch_size, seq_len, hidden_size = tensors.shape\n",
    "        filter = attentions.reshape((batch_size, seq_len, 1)).expand((batch_size, seq_len, hidden_size))\n",
    "        filtered = torch.where(filter > 0, tensors, 0)\n",
    "        l = attentions.sum(dim=1).reshape(-1, 1)\n",
    "        s = tensors.sum(dim=1)\n",
    "        return s / l\n",
    "    \n",
    "    def forward(self, input_ids, attention):\n",
    "        embedded = self.emb(input_ids)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        lm_out = self.lstm2lm(out)\n",
    "        lm_out = F.log_softmax(lm_out, dim=-1)\n",
    "        means_for_classification = self._get_means(out, attention)\n",
    "        classification_out = self.lstm2class(means_for_classification)\n",
    "        classification_out = F.log_softmax(classification_out, dim=-1)\n",
    "        return lm_out, classification_out\n",
    "\n",
    "def collate_fn(data):\n",
    "    labels = [i[1] for i in data]\n",
    "    texts = [i[0] for i in data]\n",
    "    ids = [i[2] for  i in data]\n",
    "    max_len = 5_000\n",
    "    input_ids, attentions = tokenize(texts, max_len=max_len, condense=CONDENSED_VOCAB)\n",
    "    return input_ids, attentions, torch.tensor(labels, device=device), torch.tensor(ids, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset):\n",
    "    dev_dataloader = DataLoader(dataset, shuffle=False, batch_size=4, collate_fn=collate_fn)\n",
    "    y_pred = []\n",
    "    y_gold = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attentions, labels, _ in dev_dataloader:\n",
    "            _, out = model(input_ids, attentions)\n",
    "            for i in range(out.shape[0]):\n",
    "                pred = torch.argmax(out[i]).item()\n",
    "                y_pred.append(pred)\n",
    "                y_gold.append(labels[i].item())\n",
    "    \n",
    "    r = classification_report(y_gold, y_pred, zero_division=0.0)\n",
    "    _, _, f1, _ = precision_recall_fscore_support(y_gold, y_pred, average=\"macro\", zero_division=0.0)\n",
    "    return f1, r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_checkpoint(model, optimizer, epoch, prefix=\"classifier\", report=None, progress=None):\n",
    "    dirpath = f\"checkpoints/{prefix}\"\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"report\": report,\n",
    "        \"progress\": progress\n",
    "    }\n",
    "    torch.save(checkpoint, f\"{dirpath}/epoch_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best(model, step=None, progress=None, prefix=None):\n",
    "    dirpath = f\"checkpoints/{prefix}\"\n",
    "    try:\n",
    "        os.makedirs(dirpath)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    fpath = f\"{dirpath}/latest.pt\"\n",
    "    fpath_best = f\"{dirpath}/best.pt\"\n",
    "    f1, report = evaluate(model, TaskA_Dataset(split=\"dev\"))\n",
    "\n",
    "    if \"best\" not in progress:\n",
    "        progress[\"best\"] = 0\n",
    "        \n",
    "    is_best = f1 > progress[\"best\"]\n",
    "    \n",
    "    cp = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"progress\": progress,\n",
    "        \"step\": step,\n",
    "        \"report\": report\n",
    "    }\n",
    "    \n",
    "    if is_best:\n",
    "        progress[\"best\"] = f1\n",
    "        torch.save(cp, fpath_best)\n",
    "    torch.save(cp, fpath)\n",
    "    return progress[\"best\"], f1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isnan\n",
    "\n",
    "last_batch = None\n",
    "\n",
    "def train(\n",
    "    model=None, \n",
    "    optimizer=None, \n",
    "    dataloader=None, \n",
    "    n_epochs=5, \n",
    "    start_epoch=1, \n",
    "    checkpoint_prefix=None, \n",
    "    save_every=1000,\n",
    "    progress={},\n",
    "    clip=5\n",
    "):    \n",
    "    model.train()\n",
    "    lm_criterion = nn.NLLLoss(reduction=\"mean\")\n",
    "    cl_criterion = nn.NLLLoss(reduction=\"sum\")\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    for epoch in range(start_epoch, n_epochs+1):\n",
    "        with tqdm(total=len(dataloader)) as pbar:\n",
    "            pbar.set_description(f\"Epoch {epoch}\")\n",
    "            losses = []\n",
    "            for batch in dataloader:\n",
    "                input_ids, attentions, labels, text_ids = batch\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                lm_out, classifier_out = model(input_ids, attentions)\n",
    "                loss = torch.tensor(0, dtype=torch.float32, device=device)\n",
    "                \n",
    "                # ------------------\n",
    "                # LM loss\n",
    "                # ------------------\n",
    "                \n",
    "                for i in range(input_ids.shape[0]):\n",
    "                    if labels[i].item() == 0:\n",
    "                        # only train LM on human texts\n",
    "                        until = attentions[i].cpu().tolist().index(0) if 0 in attentions[i] else -1\n",
    "                        y_pred = lm_out[i, :until]\n",
    "                        y_gold = input_ids[i, :until]\n",
    "                        y_pred = y_pred[:-1]\n",
    "                        y_gold = y_gold[1:]\n",
    "                        loss_update = lm_criterion(y_pred, y_gold)\n",
    "                        if isnan(loss_update.item()):\n",
    "                            print(\"NAN loss detected, do not backprop\")\n",
    "                            print(last_batch)\n",
    "                            return\n",
    "                        loss += loss_update\n",
    "                \n",
    "                # ------------------\n",
    "                # Classifier loss\n",
    "                # ------------------\n",
    "                loss_update = cl_criterion(classifier_out, labels)\n",
    "                loss += loss_update\n",
    "                \n",
    "                # ------------------\n",
    "                # Backprop\n",
    "                # ------------------\n",
    "                \n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                pbar.update(1)\n",
    "                last_batch = batch\n",
    "                \n",
    "                step += 1\n",
    "                if step != 0 and step % save_every == 0:\n",
    "                    best_f1, latest_f1 = save_best(model, step=step, progress=progress, prefix=checkpoint_prefix)\n",
    "                    pbar.set_postfix({\"f1\": best_f1, \"latest\": latest_f1})\n",
    "\n",
    "            print(\"LOSS\", sum(losses) / len(losses))\n",
    "            _, report = evaluate(model, TaskA_Dataset(split=\"dev\"))\n",
    "            print(report)\n",
    "            make_checkpoint(model, optimizer, epoch, prefix=checkpoint_prefix, report=report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                                                                    | 11/8990 [00:12<2:52:14,  1.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 41\u001b[0m\n\u001b[1;32m     32\u001b[0m ds\u001b[38;5;241m.\u001b[39msplit_long_items()\n\u001b[1;32m     34\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     35\u001b[0m     ds,\n\u001b[1;32m     36\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     37\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     38\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 41\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 63\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataloader, n_epochs, start_epoch, checkpoint_prefix, save_every, progress, clip)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# ------------------\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# ------------------\u001b[39;00m\n\u001b[1;32m     62\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 63\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     66\u001b[0m pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/base-env/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/base-env/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CharLM(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=512,\n",
    "    num_layers=2\n",
    ")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=0.005)\n",
    "start_epoch = 1\n",
    "checkpoint_prefix=\"charlm_512_2_fullvocab\"\n",
    "progress = {}\n",
    "\n",
    "cp_file = None\n",
    "CP = f\"checkpoints/{cp_file}\" if cp_file else None\n",
    "\n",
    "if CP:\n",
    "    checkpoint_data = torch.load(CP)\n",
    "    model.load_state_dict(checkpoint_data[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint_data[\"optimizer\"])\n",
    "    start_epoch = checkpoint_data[\"epoch\"] + 1\n",
    "    progress = checkpoint_data.get(\"progress\", {})\n",
    "    print(\"-------------------------\")\n",
    "    print(\"CHECKPOINT MODEL EVAL\")\n",
    "    print(\"-------------------------\")\n",
    "    if \"report\" in checkpoint_data:\n",
    "        print(checkpoint_data[\"report\"])\n",
    "    else:\n",
    "        evaluate(model, TaskA_Dataset(split=\"dev\"))\n",
    "    print()\n",
    "\n",
    "\n",
    "ds = TaskA_Dataset(split=\"train\")\n",
    "ds.split_long_items()\n",
    "\n",
    "loader = DataLoader(\n",
    "    ds,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "losses = train(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    dataloader=loader, \n",
    "    n_epochs=10, \n",
    "    start_epoch=start_epoch, \n",
    "    checkpoint_prefix=checkpoint_prefix,\n",
    "    save_every=2000,\n",
    "    progress=progress,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
