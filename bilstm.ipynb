{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, dataloader\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from typing import List, Union\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4530033, 300])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "with open(\"wiki2vec/dump_t.pkl\", \"rb\") as f:\n",
    "    word2idx, idx2word, embeddings = pickle.load(f)\n",
    "    \n",
    "WHITESPACE = \"<WS>\"\n",
    "word2idx[WHITESPACE] = len(word2idx)\n",
    "idx2word[word2idx[WHITESPACE]] = WHITESPACE\n",
    "dim = embeddings.shape[1]\n",
    "ws_emb = torch.randn(1, dim)\n",
    "embeddings = torch.cat([embeddings, ws_emb], dim=0)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskC(Dataset):\n",
    "    def __init__(self, split=\"train\", enrich=False, max_len=2000, word2idx=None):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.enrich = enrich\n",
    "        self.max_len = max_len\n",
    "        self.word2idx = word2idx\n",
    "        self.data = self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        f = \"data/subtaskC_train.jsonl\" if self.split == \"train\" else \"data/subtaskC_dev.jsonl\"\n",
    "        data = []\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parsed = json.loads(line)\n",
    "                data.append(parsed)\n",
    "                \n",
    "        if self.split == \"train\" and self.enrich:\n",
    "            with open(\"data/subtaskA_train_monolingual.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    parsed = json.loads(line)\n",
    "                    text = parsed[\"text\"]\n",
    "                    is_machine = parsed[\"label\"] == 1\n",
    "                    label = 0 if is_machine else -1\n",
    "                    data.append({\n",
    "                        \"text\": text,\n",
    "                        \"label\": label\n",
    "                    })\n",
    "        return data\n",
    "    \n",
    "    def _clean(self, word):\n",
    "        # all non alphanumeric\n",
    "        replace = re.compile(r\"[^a-zA-Z0-9\\-]\")\n",
    "        word = replace.sub(\"\", word)\n",
    "        return word.lower().strip()\n",
    "    \n",
    "    def _is_whitespace(self, word):\n",
    "        pat = re.compile(r\"^\\s*$\")\n",
    "        return pat.match(word) is not None\n",
    "\n",
    "    def tokenize(self, text) -> List[int]:\n",
    "        tokens = text.split(\" \")\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            token = self._clean(token)\n",
    "            if self._is_whitespace(token):\n",
    "                ids.append(self.word2idx[WHITESPACE])\n",
    "            elif token in self.word2idx:\n",
    "                ids.append(self.word2idx[token])\n",
    "            else:\n",
    "                ids.append(self.word2idx[UNK])\n",
    "        if len(ids) > self.max_len:\n",
    "            ids = ids[:self.max_len]\n",
    "        elif len(ids) < self.max_len:\n",
    "            ids = ids + ([self.word2idx[PAD]] * (self.max_len - len(ids)))\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "    \n",
    "    def get_label_vector(self, boundary:int):\n",
    "        base = [0] * self.max_len\n",
    "        if boundary > -1:\n",
    "            for i in range(boundary, self.max_len):\n",
    "                base[i] = 1\n",
    "        return torch.tensor(base, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        text = self.tokenize(data[\"text\"])\n",
    "        label = self.get_label_vector(data[\"label\"])\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Labeller(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained_embeddings = None,\n",
    "        hidden_size = 256,\n",
    "        num_layers = 1\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.emb = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.emb.cpu()\n",
    "        self.input_size = self.emb.weight.shape[1]\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm.to(device)\n",
    "        # multiply by 2 because of bidirectional\n",
    "        self.classifier_head = nn.Linear(self.hidden_size * 2, 2)\n",
    "        self.classifier_head.to(device)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        inputs: torch.Tensor = self.emb(input_ids)\n",
    "        inputs = inputs.to(device)\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        logits = self.classifier_head(outputs)\n",
    "        predicted = F.log_softmax(logits, dim=-1)\n",
    "        return predicted\n",
    "    \n",
    "    def predict(self, input_ids) -> List[int]:\n",
    "        predicted = self.forward(input_ids)\n",
    "        predicted = torch.argmax(predicted, dim=-1)\n",
    "        p = predicted.cpu().numpy().tolist()\n",
    "        r = []\n",
    "        for item in p:\n",
    "            try:\n",
    "                r.append(item.index(1))\n",
    "            except ValueError:\n",
    "                r.append(len(item) // 2)\n",
    "        return r\n",
    "    \n",
    "model = BiLSTM_Labeller(\n",
    "    pretrained_embeddings=embeddings\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_position(pred: torch.Tensor) -> int:\n",
    "    pred_np: np.ndarray = pred.detach().numpy()\n",
    "    result = []\n",
    "    for labels in pred_np:\n",
    "        try:\n",
    "            result.append(labels.tolist().index(1))\n",
    "        except ValueError:\n",
    "            result.append(len(labels) // 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskC_dev = TaskC(split=\"dev\", word2idx=word2idx)\n",
    "dev_dataloader = dataloader.DataLoader(taskC_dev, batch_size=32, shuffle=False)\n",
    "\n",
    "def eval(model):\n",
    "    pred = []\n",
    "    gold = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, labels in dev_dataloader:\n",
    "            labels = get_start_position(labels)\n",
    "            predicted = model.predict(input_ids)\n",
    "            pred.extend(predicted)\n",
    "            gold.extend(labels)\n",
    "    distances = [abs(pred[i] - gold[i]) for i in range(len(pred))]\n",
    "    print(\"MEAN ABSOLUTE DISTANCE:\", sum(distances) / len(distances))\n",
    "    return pred, gold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_checkpoint(model, optimizer, epoch):\n",
    "    try:\n",
    "        os.mkdir(\"checkpoints\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    torch.save(checkpoint, f\"checkpoints/checkpoint_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, epochs+1): \n",
    "        losses = []\n",
    "        with tqdm(dataloader, total=len(dataloader)) as pbar:\n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids, labels = batch\n",
    "                # do not put inputs to cuda, because emb is on cpu\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(input_ids)\n",
    "                # put outputs in shape (batch_size * seq_len, 2)\n",
    "                outputs = outputs.reshape(-1, 2)\n",
    "                # put labels in shape (batch_size * seq_len)\n",
    "                labels = labels.reshape(-1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.update(1)\n",
    "        print(f\"\\n---- EVALUATING MODEL at epoch {epoch} ----\")\n",
    "        print(\"LOSS:\", sum(losses) / len(losses))\n",
    "        eval(model)\n",
    "        print(\"------------------------------------------\\n\")\n",
    "        # make_checkpoint(model, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/115 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 1 ----\n",
      "LOSS: 0.09684148352269245\n",
      "MEAN ABSOLUTE DISTANCE: 64.409900990099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 2 ----\n",
      "LOSS: 0.0394621753498264\n",
      "MEAN ABSOLUTE DISTANCE: 22.102970297029703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 3 ----\n",
      "LOSS: 0.031010249799684336\n",
      "MEAN ABSOLUTE DISTANCE: 20.673267326732674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 4 ----\n",
      "LOSS: 0.025856820996040884\n",
      "MEAN ABSOLUTE DISTANCE: 31.326732673267326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 5 ----\n",
      "LOSS: 0.02171252590968557\n",
      "MEAN ABSOLUTE DISTANCE: 15.497029702970297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:02<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 6 ----\n",
      "LOSS: 0.019190575700739156\n",
      "MEAN ABSOLUTE DISTANCE: 19.176237623762376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:00<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 7 ----\n",
      "LOSS: 0.01658806996015103\n",
      "MEAN ABSOLUTE DISTANCE: 15.647524752475247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:00<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 8 ----\n",
      "LOSS: 0.015913577426386917\n",
      "MEAN ABSOLUTE DISTANCE: 15.146534653465347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 9 ----\n",
      "LOSS: 0.014954867282801348\n",
      "MEAN ABSOLUTE DISTANCE: 13.24950495049505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [02:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- EVALUATING MODEL at epoch 10 ----\n",
      "LOSS: 0.014239070021911807\n",
      "MEAN ABSOLUTE DISTANCE: 14.752475247524753\n"
     ]
    }
   ],
   "source": [
    "loader = dataloader.DataLoader(\n",
    "    TaskC(word2idx=word2idx, split=\"train\"),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, loader, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
