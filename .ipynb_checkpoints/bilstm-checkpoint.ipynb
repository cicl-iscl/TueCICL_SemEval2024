{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, dataloader\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from typing import List, Union\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f858359ed10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4530033, 500])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "with open(\"wiki2vec/dump_500_t.pkl\", \"rb\") as f:\n",
    "    word2idx, idx2word, embeddings = pickle.load(f)\n",
    "    \n",
    "WHITESPACE = \"<WS>\"\n",
    "word2idx[WHITESPACE] = len(word2idx)\n",
    "idx2word[word2idx[WHITESPACE]] = WHITESPACE\n",
    "dim = embeddings.shape[1]\n",
    "ws_emb = torch.randn(1, dim)\n",
    "embeddings = torch.cat([embeddings, ws_emb], dim=0)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskC(Dataset):\n",
    "    def __init__(self, split=\"train\", use_scaled=False, max_len=2000, word2idx=None):\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.use_scaled = use_scaled\n",
    "        self.max_len = max_len\n",
    "        self.word2idx = word2idx\n",
    "        self.data, self.perplexities = self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        f = \"data/subtaskC_train.jsonl\" if self.split == \"train\" else \"data/subtaskC_dev.jsonl\"\n",
    "        ppl_f_train = \"data/ppl_train.json\" if not self.use_scaled else \"data/ppl_train_scaled.json\"\n",
    "        ppl_f_dev = \"data/ppl_dev.json\" if not self.use_scaled else \"data/ppl_dev_scaled.json\"\n",
    "        ppl_f = ppl_f_train if self.split == \"train\" else ppl_f_dev\n",
    "        data = []\n",
    "        \n",
    "        with open(f, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parsed = json.loads(line)\n",
    "                data.append(parsed)\n",
    "        with open(ppl_f, \"r\", encoding=\"utf-8\") as f:\n",
    "            ppl_data = json.load(f)\n",
    "                \n",
    "        \n",
    "        return data, ppl_data\n",
    "    \n",
    "    def _clean(self, word):\n",
    "        # all non alphanumeric\n",
    "        replace = re.compile(r\"[^a-zA-Z0-9\\-]\")\n",
    "        word = replace.sub(\"\", word)\n",
    "        return word.lower().strip()\n",
    "    \n",
    "    def _is_whitespace(self, word):\n",
    "        pat = re.compile(r\"^\\s*$\")\n",
    "        return pat.match(word) is not None\n",
    "\n",
    "    def tokenize(self, text) -> List[int]:\n",
    "        tokens = text.split(\" \")\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            token = self._clean(token)\n",
    "            if self._is_whitespace(token):\n",
    "                ids.append(self.word2idx[WHITESPACE])\n",
    "            elif token in self.word2idx:\n",
    "                ids.append(self.word2idx[token])\n",
    "            else:\n",
    "                ids.append(self.word2idx[UNK])\n",
    "        if len(ids) > self.max_len:\n",
    "            ids = ids[:self.max_len]\n",
    "        elif len(ids) < self.max_len:\n",
    "            ids = ids + ([self.word2idx[PAD]] * (self.max_len - len(ids)))\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "    \n",
    "    def get_perplexities(self, idx):\n",
    "        ppl = self.perplexities[idx]\n",
    "        \n",
    "        if len(ppl) > self.max_len:\n",
    "            ppl = ppl[:self.max_len]\n",
    "        elif len(ppl) < self.max_len:\n",
    "            last = ppl[-1]\n",
    "            pad = [0.0] * len(last)\n",
    "            ppl = ppl + ([ppl] * (self.max_len - len(ppl)))\n",
    "        return torch.tensor(ppl)\n",
    "    \n",
    "    def get_label_vector(self, boundary:int):\n",
    "        base = [0] * self.max_len\n",
    "        if boundary > -1:\n",
    "            for i in range(boundary, self.max_len):\n",
    "                base[i] = 1\n",
    "        return torch.tensor(base, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        text = self.tokenize(data[\"text\"])\n",
    "        ppl = self.get_perplexities(index)\n",
    "        label = self.get_label_vector(data[\"label\"])\n",
    "        return text, ppl, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Labeller(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        pretrained_embeddings = None,\n",
    "        hidden_size = 1024,\n",
    "        num_layers = 2,\n",
    "        feature_vector_dim = 0\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.emb = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.emb.cpu()\n",
    "        self.emb.weight.required_grad = False\n",
    "        self.input_size = self.emb.weight.shape[1] + feature_vector_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.lstm.to(device)\n",
    "        # multiply by 2 because of bidirectional\n",
    "        self.classifier_head = nn.Linear(self.hidden_size * 2, 2)\n",
    "        self.classifier_head.to(device)\n",
    "        \n",
    "    def forward(self, input_ids, feature_vectors):\n",
    "        inputs: torch.Tensor = self.emb(input_ids)\n",
    "        # inputs = torch.cat((inputs, feature_vectors), dim=-1)\n",
    "        # inputs = F.normalize(inputs)\n",
    "        inputs = inputs.to(device)\n",
    "        outputs, _ = self.lstm(inputs)\n",
    "        logits = self.classifier_head(outputs)\n",
    "        predicted = F.log_softmax(logits, dim=-1)\n",
    "        return predicted\n",
    "    \n",
    "    def predict(self, input_ids, feature_vectors) -> List[int]:\n",
    "        predicted = self.forward(input_ids, feature_vectors)\n",
    "        predicted = torch.argmax(predicted, dim=-1)\n",
    "        p = predicted.cpu().numpy().tolist()\n",
    "        r = []\n",
    "        for item in p:\n",
    "            try:\n",
    "                r.append(item.index(1))\n",
    "            except ValueError:\n",
    "                r.append(len(item) // 2)\n",
    "        return r\n",
    "    \n",
    "model = BiLSTM_Labeller(\n",
    "    pretrained_embeddings=embeddings\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_position(pred: torch.Tensor) -> int:\n",
    "    pred_np: np.ndarray = pred.detach().numpy()\n",
    "    result = []\n",
    "    for labels in pred_np:\n",
    "        try:\n",
    "            result.append(labels.tolist().index(1))\n",
    "        except ValueError:\n",
    "            result.append(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskC_dev = TaskC(split=\"dev\", word2idx=word2idx, use_scaled=False)\n",
    "dev_dataloader = dataloader.DataLoader(taskC_dev, batch_size=32, shuffle=False)\n",
    "\n",
    "def eval(model):\n",
    "    pred = []\n",
    "    gold = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, perplexities, labels in dev_dataloader:\n",
    "            labels = get_start_position(labels)\n",
    "            predicted = model.predict(input_ids, perplexities)\n",
    "            pred.extend(predicted)\n",
    "            gold.extend(labels)\n",
    "    distances = [abs(pred[i] - gold[i]) for i in range(len(pred))]\n",
    "    print(\"MEAN ABSOLUTE DISTANCE:\", sum(distances) / len(distances))\n",
    "    return pred, gold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_checkpoint(model, optimizer, epoch):\n",
    "    try:\n",
    "        os.mkdir(\"checkpoints\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "    }\n",
    "    torch.save(checkpoint, f\"checkpoints/checkpoint_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(1, epochs+1): \n",
    "        losses = []\n",
    "        with tqdm(dataloader, total=len(dataloader)) as pbar:\n",
    "            for batch in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                input_ids, perplexities, labels = batch\n",
    "                # do not put inputs to cuda, because emb is on cpu\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(input_ids, perplexities)\n",
    "                # put outputs in shape (batch_size * seq_len, 2)\n",
    "                outputs = outputs.reshape(-1, 2)\n",
    "                # put labels in shape (batch_size * seq_len)\n",
    "                labels = labels.reshape(-1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                losses.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                pbar.update(1)\n",
    "        print(f\"\\n---- EVALUATING MODEL at epoch {epoch} ----\")\n",
    "        print(\"LOSS:\", sum(losses) / len(losses))\n",
    "        eval(model)\n",
    "        print(\"------------------------------------------\\n\")\n",
    "        # make_checkpoint(model, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aron/base-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [04:48<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- EVALUATING MODEL at epoch 1 ----\n",
      "LOSS: 0.07940707621367081\n",
      "MEAN ABSOLUTE DISTANCE: 35.46534653465346\n",
      "------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [04:48<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- EVALUATING MODEL at epoch 2 ----\n",
      "LOSS: 0.043004452890676004\n",
      "MEAN ABSOLUTE DISTANCE: 55.88514851485149\n",
      "------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [04:49<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- EVALUATING MODEL at epoch 3 ----\n",
      "LOSS: 0.046447067238066506\n",
      "MEAN ABSOLUTE DISTANCE: 29.704950495049506\n",
      "------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [04:48<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- EVALUATING MODEL at epoch 4 ----\n",
      "LOSS: 0.02891164211475331\n",
      "MEAN ABSOLUTE DISTANCE: 48.96633663366337\n",
      "------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 115/115 [04:48<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- EVALUATING MODEL at epoch 5 ----\n",
      "LOSS: 0.03216436421093733\n"
     ]
    }
   ],
   "source": [
    "loader = dataloader.DataLoader(\n",
    "    TaskC(word2idx=word2idx, split=\"train\", use_scaled=False),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, loader, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
