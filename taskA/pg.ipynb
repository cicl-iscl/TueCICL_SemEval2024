{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aron/cicl/taskA/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"rrivera1849/LUAR-CRUD\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"rrivera1849/LUAR-CRUD\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', 'H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', 'H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', ' ', 't', 'e', 'w', 't', ' ', 'w', 'e', 'g', 'e', 'r', 'g', 't', ' ', 'e', 'r', 't', ' ', 'e', 'r', 't', ' ']\n",
      "torch.Size([55, 32])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 16, -1]' is invalid for input of size 1760",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# inputs size: (batch_size, episode_length, max_token_length)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m tokenized_text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized_text\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenized_text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     23\u001b[0m tokenized_text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenized_text[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(batch_size, episode_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[3, 16, -1]' is invalid for input of size 1760"
     ]
    }
   ],
   "source": [
    "\n",
    "text = [\n",
    "    \"Hello world\",\n",
    "    \"Hello world\",\n",
    "    \"Hello world tewt wegergt ert ert \",\n",
    "]\n",
    "text = [j for i in text for j in i]\n",
    "print(text)\n",
    "tokenized_text = tokenizer(\n",
    "    text, \n",
    "    max_length=32,\n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "batch_size = 3\n",
    "episode_length = 16\n",
    "\n",
    "print(tokenized_text[\"input_ids\"].size())\n",
    "# inputs size: (batch_size, episode_length, max_token_length)\n",
    "tokenized_text[\"input_ids\"] = tokenized_text[\"input_ids\"].reshape(batch_size, episode_length, -1)\n",
    "print(tokenized_text[\"input_ids\"][0][2])\n",
    "tokenized_text[\"attention_mask\"] = tokenized_text[\"attention_mask\"].reshape(batch_size, episode_length, -1)\n",
    "print(tokenized_text[\"input_ids\"].size())       # torch.Size([3, 16, 32])\n",
    "print(tokenized_text[\"attention_mask\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(**tokenized_text)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2546e-01,  3.1335e-01,  2.8418e-01, -1.7627e-01, -8.1877e-02,\n",
       "         -1.8329e-01, -6.5513e-01, -1.5010e-01,  2.0820e-01,  5.8498e-02,\n",
       "         -2.5200e-01, -6.2587e-02, -2.6575e-02, -1.5212e-01,  5.7342e-03,\n",
       "         -1.2253e-01, -5.7833e-01, -2.5049e-01, -2.3850e-01, -4.3312e-01,\n",
       "          1.4292e-01, -3.3069e-01,  2.6330e-02,  9.1429e-02, -1.5016e-01,\n",
       "          3.9968e-01,  1.7292e-01, -2.0076e-01, -8.0636e-02, -2.3247e-01,\n",
       "          4.3047e-01,  3.5926e-02, -8.9328e-02,  1.1421e-01,  1.2197e-01,\n",
       "         -2.5000e-01, -4.2469e-01,  2.6042e-01,  5.0199e-02, -4.8428e-01,\n",
       "         -4.8705e-02,  5.3210e-02, -1.0499e-01,  4.8214e-01, -6.4619e-03,\n",
       "         -1.6207e-01,  3.1712e-02, -3.8620e-01,  1.4913e-01,  2.5625e-01,\n",
       "         -1.3145e-02,  5.9263e-01, -5.4286e-02, -5.8829e-01,  3.0340e-01,\n",
       "          9.0748e-03, -2.3600e-01,  3.3404e-01, -2.1046e-01, -8.2160e-02,\n",
       "         -3.5743e-03, -8.7848e-02,  1.6317e-01,  1.2717e-01, -8.3906e-02,\n",
       "          9.5846e-03, -6.5378e-02,  1.9081e-01, -5.6592e-02, -1.9946e-01,\n",
       "         -2.2783e-01, -7.1444e-02, -5.7269e-01, -1.5245e-01, -1.3001e-01,\n",
       "         -1.7315e-01, -8.1352e-02,  1.1016e-01,  6.1774e-03,  1.4638e-01,\n",
       "          2.0886e-01, -1.5713e+00, -3.7770e-02,  1.7325e-01,  1.1276e-01,\n",
       "          1.5526e-01,  1.4842e-01, -4.0783e-01, -4.3713e-01, -3.1157e-01,\n",
       "          1.0160e-01,  1.2119e-01, -1.0998e-01,  9.5871e-02,  2.5362e-01,\n",
       "          4.2093e-01, -3.7466e-01,  1.5681e-01, -3.6210e-01, -1.7236e-01,\n",
       "          1.1748e-02,  3.0856e-02,  1.6951e-01,  8.2007e-02, -4.9369e-02,\n",
       "         -2.9682e-01, -2.9700e-01, -4.0849e-01,  2.7842e-01, -3.3350e-01,\n",
       "          2.9085e-02,  2.2850e-02, -7.9193e-03, -1.3218e+00,  1.3464e-01,\n",
       "         -7.7108e-02, -5.7511e-01,  2.7193e+00,  9.0402e-02,  1.4335e-01,\n",
       "         -6.9751e-02, -1.1846e-01,  3.3901e-01,  5.1460e-01, -3.7103e-01,\n",
       "         -3.5005e-01, -4.1878e-02, -9.0391e-02,  1.5386e-01,  2.7480e-02,\n",
       "          1.4891e-01, -7.8278e-03,  1.4494e-01, -3.3385e-01, -2.6468e+00,\n",
       "          3.2001e-01, -9.3884e-02,  6.4300e-02, -3.8560e-01,  3.1852e-02,\n",
       "          3.4696e-01, -9.7152e-02,  1.2021e-01,  8.4336e-02, -7.4258e-01,\n",
       "         -2.5831e-01,  1.9492e-01,  9.1044e-02, -3.2532e-01, -1.7366e-01,\n",
       "         -3.3914e-01,  1.0516e-01,  4.3206e-01, -3.6663e-01,  1.8348e-01,\n",
       "          5.3526e-02, -1.0792e-01,  7.4023e-02, -9.4979e-02, -1.9103e-01,\n",
       "         -2.7776e-02, -2.8394e-01, -2.4319e-01, -4.5595e-02, -1.0415e-01,\n",
       "         -2.9752e-01,  3.4015e-01, -3.4315e-01, -8.8986e-02,  2.6136e+00,\n",
       "          1.4851e-01, -1.1171e-01, -1.3581e-01, -3.0459e-01, -4.1374e-02,\n",
       "          2.1249e-01,  2.3045e-01,  7.2545e-02, -3.2295e-01,  1.5330e+00,\n",
       "         -1.7682e-01,  4.9986e-02, -7.1329e-02, -3.0636e-01,  3.4889e-01,\n",
       "         -2.8362e-01,  1.2231e-01,  3.3240e-01,  4.5409e-02, -9.0967e-01,\n",
       "         -1.6833e-01, -6.5842e-02, -2.3124e-02,  3.4161e-02,  2.0519e-01,\n",
       "         -1.4876e-01,  1.3811e-01,  2.3950e-01, -4.0293e-01, -5.0098e-01,\n",
       "         -7.5397e-02, -3.2472e-01,  1.5007e-01, -3.1081e-01,  4.0317e-01,\n",
       "          3.2427e-01,  1.3172e-01, -1.9876e-01, -4.1225e-02,  1.6159e-03,\n",
       "          9.4744e-03, -8.6239e-02,  2.9417e-01,  5.2691e-01,  4.9286e-02,\n",
       "         -9.9482e-02, -1.1631e-01,  9.5132e-02,  2.1145e-02, -5.5024e-02,\n",
       "         -4.1742e-01, -3.9517e-01,  6.1416e-02,  3.4830e-02,  1.6020e-01,\n",
       "         -9.5483e-02, -6.3338e-02, -1.2952e-01,  1.4052e-01,  3.2490e-02,\n",
       "          1.7804e-02,  3.6208e-02, -4.1300e-01,  3.2241e-01, -1.6289e-01,\n",
       "         -1.5071e-02,  4.4612e-01, -2.1490e-01, -8.7570e-02, -5.0257e-03,\n",
       "         -1.6066e-03, -1.6954e-01,  1.9426e-01, -9.0289e-02, -1.4758e-01,\n",
       "         -7.6793e-02, -1.7847e-01,  3.1897e-02, -3.7631e-01, -1.4263e-01,\n",
       "         -2.9133e-01,  4.8911e-02,  1.3325e-02,  7.6880e-04,  2.0973e-01,\n",
       "         -6.1593e-02, -1.7538e-01, -2.9701e-03, -4.3854e-02,  2.0461e-02,\n",
       "          3.5404e-01,  1.4122e-01,  2.4333e-01, -1.2415e-01,  3.4794e-01,\n",
       "         -3.1891e-02, -2.1039e-01, -2.0347e-02,  1.6478e-02,  6.9233e-02,\n",
       "          3.6750e-01,  3.1655e-01, -1.0666e-01,  2.2139e-01,  1.4196e-01,\n",
       "         -6.4160e-02, -5.6340e-01,  2.9264e-01,  1.8393e-01,  1.3120e-01,\n",
       "          1.2528e-02,  1.2075e+00,  9.6466e-02, -7.7675e-02, -5.7474e-02,\n",
       "          8.1073e-02, -1.1570e-01, -9.4050e-02,  3.1870e-01,  1.9475e-01,\n",
       "          3.6896e-01,  2.1057e-01,  3.9505e-02,  1.5395e-02,  1.1484e-01,\n",
       "         -3.8039e-01,  1.6895e-01,  1.0094e-01, -1.9673e-01,  8.3311e-02,\n",
       "         -1.4194e-01,  7.5532e-02, -3.7390e-02, -8.9448e-02,  4.2062e-01,\n",
       "          2.3722e-01, -2.4384e-03,  1.3077e-01, -6.1816e-02,  4.4549e-01,\n",
       "         -1.1454e-01,  9.0170e-02, -1.5411e-01, -7.8800e-01,  2.6875e-01,\n",
       "         -2.5385e-01,  1.1479e-01, -2.4666e-01, -1.5160e-01, -2.4564e-01,\n",
       "         -3.4142e-01,  3.3733e-01,  1.8443e-01, -2.0447e-01, -2.8342e-01,\n",
       "          2.4586e-01,  2.8180e-01, -4.9210e-02, -3.5871e-01,  9.6744e-02,\n",
       "          4.7791e-01,  4.0715e-03, -3.5169e-02, -1.1626e-02, -2.0199e-01,\n",
       "          3.1270e-01, -1.6144e-02,  8.6216e-02,  3.6186e-01,  7.9488e-02,\n",
       "         -5.5518e-03,  3.1065e-01, -4.8948e-01,  1.1299e-01,  2.8932e-04,\n",
       "         -1.1677e-02,  1.9810e-01,  1.4308e-01, -3.4044e-01,  1.5149e+00,\n",
       "          2.7367e-01,  4.1624e-02, -6.5755e-01, -1.1288e-01,  1.1764e-02,\n",
       "          7.8160e-02,  2.4216e-02,  3.9295e-02, -3.4636e-01,  3.7761e-01,\n",
       "          1.9454e-01, -2.5098e-01, -3.3464e-01,  3.2781e-01, -9.5182e-02,\n",
       "          3.2533e-01,  1.3151e-01,  1.2652e-01,  3.2658e-01, -3.3049e-01,\n",
       "          1.7919e-01, -3.9121e-01,  1.9697e+00, -4.6993e-01, -3.2340e-01,\n",
       "         -1.4264e-01, -1.0029e+00,  1.7248e-01,  3.7918e-02,  9.7048e-02,\n",
       "         -7.3425e-01, -2.3318e-01, -5.5694e-02,  2.0795e+00,  6.1749e-02,\n",
       "         -4.5356e-02,  2.5871e-01,  1.2719e-02, -3.3989e-03,  1.1851e-01,\n",
       "         -2.0089e-01, -1.4484e-01, -1.6613e-01, -2.7223e-01,  1.3707e-03,\n",
       "         -1.2620e-01, -2.5176e-01,  4.7090e-02,  7.9254e-02,  1.6474e+00,\n",
       "          1.0258e-01,  2.6249e-01,  1.2730e+00,  3.2349e-01, -4.4454e-02,\n",
       "         -2.8050e-02,  4.3418e-01,  3.9100e-02,  2.1293e-02, -3.3289e-02,\n",
       "          2.7499e-02,  2.1574e-01, -1.1858e-01, -2.5618e-01,  1.6620e-01,\n",
       "          4.3651e-01, -3.0607e-01, -7.2019e-02, -1.3255e-01, -1.3786e-01,\n",
       "         -4.8181e-01, -1.1849e-01, -1.6624e-01, -1.1055e-01, -5.8710e-02,\n",
       "          2.1037e-01, -3.8568e-01,  5.3797e-02,  7.4593e-02, -1.2918e-01,\n",
       "         -1.7498e-01,  2.7757e-01, -1.4399e+00, -6.4553e-02,  6.8539e-02,\n",
       "         -5.0319e-02,  4.0940e-01,  1.3222e-01, -5.3251e-01,  4.1700e-02,\n",
       "         -2.5930e-01,  2.7675e-01,  2.7011e-01,  3.7907e-01,  5.4903e-04,\n",
       "         -2.8414e-01,  4.9467e-02,  1.2131e+00,  2.6581e-01, -2.0741e-01,\n",
       "         -1.0359e-01, -9.8075e-02,  5.2188e-01, -3.8688e-02,  2.2995e-01,\n",
       "         -1.1134e-01,  9.3079e-03,  8.0455e-02,  1.2788e-01,  1.1268e-01,\n",
       "          1.8155e-01,  2.7010e-01,  1.1915e-01, -2.1866e-01,  1.3154e-01,\n",
       "         -3.2191e-01, -1.0256e-01,  1.7829e-01,  1.3819e-01,  2.0044e-01,\n",
       "         -1.9563e+00,  4.9973e-01, -2.7327e-01, -1.0894e-01,  3.1250e-01,\n",
       "          2.7738e-01,  2.9928e-01, -5.6089e-02, -2.2812e-01,  3.2240e-02,\n",
       "         -1.5130e-01, -1.2514e-02,  1.7256e-02,  2.3243e-01, -1.6949e-02,\n",
       "          8.9090e-02,  2.5417e-01,  3.7240e-02, -9.2218e-02, -4.1932e-01,\n",
       "         -4.8015e-01, -3.1551e-02, -7.7813e-02, -1.5166e-02,  1.6209e+00,\n",
       "          5.5229e-02,  9.2965e-02,  5.2862e-01, -2.5579e-02, -2.8124e-02,\n",
       "          7.6164e-02,  1.2445e-01, -2.1650e-01, -2.5237e-01,  4.0072e-02,\n",
       "         -4.4117e-01, -4.6620e-03, -3.6184e-01, -4.8092e-01, -1.6133e-01,\n",
       "         -2.7993e-01,  1.0848e-02]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    \"hello world\".split()\n",
    "]\n",
    "\n",
    "batch_size = 1\n",
    "episode_length = len(text[0])\n",
    "text = [j for i in text for j in i]\n",
    "\n",
    "tokenized_text = tokenizer(\n",
    "    text, \n",
    "    max_length=32,\n",
    "    padding=\"max_length\", \n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "tokenized_text[\"input_ids\"] = tokenized_text[\"input_ids\"].reshape(batch_size, episode_length, -1)\n",
    "tokenized_text[\"attention_mask\"] = tokenized_text[\"attention_mask\"].reshape(batch_size, episode_length, -1)\n",
    "\n",
    "\n",
    "out = model(**tokenized_text)\n",
    "out.shape\n",
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
